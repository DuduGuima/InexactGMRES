
\section{Low-rank Matrices}
    
In reality, most matrices are big, so storing each element is not efficient, or even possible. If $A \in \mathbb{C}^{n\times m}$ has a rank \textit{k} such that $k \leq m$ and $k(n+m) < n*m$ (\textit{A} is low-rank), \textit{A} can be written in outer product form, as a product between the matrices $U \in \mathbb{C}^{n\times k} $ and $V \in \mathbb{C}^{m\times k}$, which can be see in \ref{eq::outer_product}, where $u_{i}, v_{i}$ are the column vectors of \textit{U} and \textit{V}.

\begin{equation}\label{eq::outer_product}
    A = UV^{H} = \sum_{i=1} ^{k} u_{i} v_{i} ^{*}
\end{equation}


Therefore, storing $k(n+m)$ elements to write $A$, and not $n\times m$. A matrix \textit{A} that can be represented as \ref{eq::outer_product} is an element of $\mathbb{C}^{n\times m}_{k}$.

The representation in \ref{eq::outer_product} also facilitates other operations with $A$, like matrix-vector products $Ab$ that are always present in methods like GMRES \cite{bebendorf2008hierarchical} and different kinds of norms, like $\norm{A}_{F}, \norm{A}_{2}$ \cite{bebendorf2008hierarchical}. 

However, even full rank matrices can be approximated by matrices with lower rank. A theorem \cite{bebendorf2008hierarchical} establishes that the closest matrix from $\mathbb{C}^{n\times m}_{k}$ of a matrix from $\mathbb{C}^{n\times m}$ can be obtained from the SVD $A = U \Sigma V^{H}$, where $\Sigma$ contains the singular valuers $\sigma_{1} \geq \sigma_{2} \dots \sigma_{m} \geq 0$ and $U,V$ are unitary.

If $ A_{k} $ is the approximation obtained after taking the first k elements of $\Sigma$ (creating the matrix $ \Sigma_{k} $ ), the error between $ A $ and $ A_{k} $ is \ref{eq:error_lowrank}.

\begin{equation}\label{eq:error_lowrank}
    \norm{A-A_{k}} = \norm{U\Sigma V^{H} - U^{'} \Sigma_{k} V_{' H}} = \norm{\Sigma - \Sigma_{k}} 
\end{equation}

If the spectral norm , $\norm{.}_{2} $ is used instead, the error in \ref{eq:error_lowrank} is given by $\sigma_{k+1}$. For Frobenius's norm, $\norm{.}_{F}$, the error becomes $\sum^{n}_{l=k+1} \sigma^{2}_{l}$.

Instead of approximating big matrices entirely, it's better to think in approximations made to each of their blocks. Blocks that appear after the discretization of elliptic operators also have the possibility of being approximated by matrices that decay exponentially with k, $S_{k}$, as in \ref{eq:exp_matrix}.

\begin{equation}\label{eq:exp_matrix}
    \norm{A-S_{k}}_{2} < q^{k}\norm{A}_{2}
\end{equation}


That way, the rank and the precision are related in a logarithmic manner, and the rank required by a certain $\epsilon$ is \ref{eq:exp_error}.

\begin{equation}\label{eq:exp_error}
    k(\epsilon) = min\{ k \in \mathbb{N} : \sigma_{k+1} < \epsilon\sigma_{1}\}
\end{equation}

\section{ACA Method(Adaptative Cross Approximation)}

As shown in the last section, the SVD methods gives us an approximation of $A$ given a certain $\epsilon$, through the relation in \ref{eq:error_lowrank}. Nevertheless, this is an expensive method, where the complexity becomes too big for some calculations.

The algorithm for the method is in \ref{alg:aca_method}, where $a_{ij}$ are the elements of a matrix $A \in \mathbb{R}^{n\times m}$. The main objective is to approximate $A$ as $A=S_{k} + R_{k}$, $S_{k} = \sum_{l=1}^{k} u_{l}v_{l}^{t}$ and $R_{k}$ is the residue.



\begin{algorithm}
\caption{ACA Method}\label{alg:aca_method}
\begin{algorithmic}[1]
\State $k=1$ et $\mathbf{Z} = \emptyset $
\Repeat
\State TFind $i_{k}$
\State $\hat{v}_{k} = a_{i_{k},1:m} $
\For{$l=1,\dots , k-1$}
\State $\hat{v}_{k} = \hat{v}_{k} - (u_{l})_{i_{k}}v_{l} $
\EndFor
\State $Z = Z \bigcup \{ i_{k} \} $

\If{$\hat{v}_{k}$ doesn't disappear}
\State $j_{k} = argmax_{j}|(\hat{v}_{k})_{j}|$ ; $v_{k} = (\hat{v}_{k})^{-1}_{j_{k}} \hat{v}_{k}$
\State $u_{k}=a_{1:n,j_{k}}$

\For{$l=1,\dots,k-1$}
\State $u_{k}=u_{k} - (v_{l})_{j_{k}}u_{l}$
\EndFor
\State $k=k+1$

\EndIf


\Until{$\norm{u_{k}}\norm{v_{k}} \leq \epsilon$}

\end{algorithmic}
\end{algorithm}

Considering $I,J \in \mathbb{N}$ the index set of a given matrix and $\mathbf{T}_{I \times J}$ the cluster block tree that contains an admissible partition \textit{P} of $ I \times J$ in its leaves, $\mathfrak{L}(\mathbf{T}_{I \times J})$. The set of hierarchical matrices in $\mathbf{T}_{I \times J}$ rank k for each block $A_{b}$ defined in \ref{eq:matrix_hier}.

\begin{equation}\label{eq:matrix_hier}
    \mathfrak{H}(\mathbf{T}_{I \times J},k) = \left\{  A\in \mathbb{C}^{I\times J} : rankA_{b} \leq k, \forall b \in P \right\}
\end{equation}
