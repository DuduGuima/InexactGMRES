\documentclass[a4paper, 15pt]{report}

\usepackage[top=4cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{algorithm,algpseudocode}
\usepackage[english]{babel}
\usepackage{iflang}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}

\usepackage{graphicx}
\usepackage[pdftex]{hyperref}
\usepackage{url}
\usepackage{svg}

\usepackage{color, xcolor, colortbl}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{makecell}
\usepackage{ragged2e}
\usepackage{sectsty}
\usepackage{setspace}
\usepackage{slantsc}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}

% ------------------------------------------

\graphicspath{ {images/} }

\setstretch{1.25}

% ------------------------------------------

\renewcommand{\indent}[1]{\setlength{\parindent}{#1}}

\newcommand{\sepline}{\vspace{-1mm} \line(1,0){60} \vspace{-1mm}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% ------------------------------------------

\pagestyle{fancy}
\lhead{Projet de Recherche}
\rhead{ENSTA Paris 2A 2023\hspace{0.5mm}-24}
\lfoot{Matrices Hiérarchiques et GMRES Inexacte}
\rfoot{\thepage}
\cfoot{}

\renewcommand{\footrulewidth}{0.4pt}

% ------------------------------------------

%% Document
\begin{document}
    \selectlanguage{english}

    \thispagestyle{empty}
    \addtocounter{page}{-1}

    \indent{0mm} \newgeometry{top=3cm, bottom=2cm}

    \begin{minipage}{\linewidth}
        \includesvg[width=8cm]{images/ensta_logo.svg}
    \end{minipage}

    \vspace{5cm}

    \begin{center}
        \fontsize{16pt}{24pt} \selectfont{\textbf{Matrices Hiérarchiques et \\ GMRES Inexacte}}

        \vspace{1.8cm} \line(1,0){60} \vspace{2cm}

        \fontsize{12pt}{18pt} \selectfont{
            \begin{tabular}{rl}
                GUIMARÃES LINO DE PAULA & Eduardo 
            \end{tabular}

        \vfill Palaiseau, le 13 mai 2024}
    \end{center}

    \newpage \restoregeometry

    \tableofcontents \newpage

    \chapter{Méthodes itératives et Sous-espace de Krylov }
    \section{Méthodes itératives et motivation}
    Les méthodes itératives apparaissent comme une alternative aux méthodes de solution directe , où la vrai solution n'est pas recherchée et une bonne approximation suffit.

    L'idée consiste à trouver, après un nombre définit d'itérations, une suite ${x_{k}}$ qui converge à $x$, la solution exacte du problème \ref{eq:suite}.

    \begin{equation}\label{eq:suite}
        x = lim_{k \to \infty} x_{k}
    \end{equation}

    La méthode est appliquée de façon à s'arrêter après $k$ itérations, où $x_{k}$ est le premier élément de la suite à satisfaire la condition \ref{eq:ch1_it}.

    \begin{equation}\label{eq:ch1_it}
       \frac{||x_{k} - x||}{||x||} \leq \epsilon
    \end{equation}

    Où $\epsilon$ est une tolérance définie par qui l'applique.

    Normalement $x$ n'est pas connue, de façon que \ref{eq:ch1_it} est changée pour \ref{eq:residual}, où $A$ est la matrice du système linéaire et $b$ le RHS(Right Hand Side).

    \begin{equation}\label{eq:residual}
        \frac{||Ax_{k} - b||}{||b||} \leq \epsilon
    \end{equation}

    Les premiers méthodes itératives utilisaient une décomposition de la matrice $A$ comme une combinaison de deux matrices \ref{eq:A-comb}, où $A_{1}$ est inversible, et chaque itération serait définie comme \ref{eq:A_it}.

    \begin{equation}\label{eq:A-comb}
        A = A_{1} - A_{2}
    \end{equation}

    \begin{equation}\label{eq:A_it}
        A_{1} x_{k+1} = b + A_{2}x_{k}
    \end{equation}

    Avec une substitution des autres $x_{k}$, \ref{eq:A_it} donne \ref{eq:it_fin}, qui converge n'importe quelle solution initiale ssi $\rho(A_{2}A_{1}^{-1}) < 1$, où $\rho(X)$ est le rayon spectral de la matrice X \cite{bonnet}.

    \begin{equation}\label{eq:it_fin}
        x_{k+1} = A_{1}^{-1}(b + A_{2}x_{k}) = A_{1}^{-1}(b + A_{2}A_{1}^{-1}(b + A_{2}x_{k-1}))... = A_{1}^{-1} \left[ \sum_{i=0}^{k} (A_{2}A_{1}^{-1})^{i}b\right]
    \end{equation}

    Si $A_{1} = I$ et $A_{2} = I - A$ en \ref{eq:A-comb}, la suite trouvée en \ref{eq:it_fin} est: $x_{1} = b$,$x_{2} = 2b- Ab$, $x_{3} = 3b-3Ab+A^{2}b$ , ...

    Même que la condition $\rho(A-I) \leq 1$ soit restrictive \cite{bonnet}, cela nous montre qu'une approximation $x_{k}$ peut être représentée comme \ref{eq:xkry}.

    \begin{equation}\label{eq:xkry}
        x_{k} \in span(b,Ab,A^{2}b,...,A^{k-1}b)
    \end{equation}

    \section{Sous-espace de Krylov}
    Soit $A \in \mathbb{K}^{n \times n}$ une matrice et $b\in \mathbb{K}^{n}$. Pour $k\leq n$ le sous-espace de Krylov $\mathcal{K}_{k}=\mathcal{K}_{k}(A,b)$ associé à A,b est défini comme \ref{eq:krylov}.

    \begin{equation}\label{eq:krylov}
        \mathcal{K}_{k}(A,b) = span(b,Ab,A^{2}b,\dots , A^{k-1}b)
    \end{equation}

    Ces sous-espaces suivent aussi la propriété: $k<l \to \mathcal{K}^{k} \subset \mathcal{K}^{l}$ \cite{bonnet}.

    Ce sous-espace $\mathcal{K}_{k}(A,b)$ est aussi le sous-espace de tous les vecteurs de $\mathbb{R}^{m}$ qui peuvent être écrits comme $x=p(A)b$, où $p(A)$ est un polynôme de degré inférieur à $k-1$ dont $p(0)=1$.

    Le problème avec l'utilisation de ${A^{k}b}, k \in {0,1,2,\dots}$ comme une base vient du fait que les produits successifs de la matrice $A$ font des vecteurs qui sont \textit{presque colinéaires}, vu que ceux sont proches du vecteur propre du plus grand valeur propre de la matrice $A$.

    \section{Méthode d'Arnoldi}
    
    Dans le but d'obtenir une base orthonormale pour $\mathcal{K}_{k}(A,b)$, le méthode cherche une matrice unitaire $Q$ tel que l'expression \ref{eq:init_arnoldi} est valide. $H_{k}={h_{ij}}$ est une matrice de Hessenberg.

    \begin{equation} \label{eq:init_arnoldi}
        AQ_{k} = Q_{k+1}H_{k}
    \end{equation} 

    Pour chaque vecteur-colonne de $Q$, $q_{i}$, \ref{eq:init_arnoldi} peut être écrite comme \ref{eq:final_arnoldi}, où la représentation de $\mathcal{K}_{k}(A,b)$ avec une base orthonormal devient plus claire. Dans une application pratique, $Q$ est initialisée avec $q_{1} = \frac{b}{||b||}$.

    \begin{equation}\label{eq:final_arnoldi}
        Aq_{m} = h_{1m}q_{1} + h_{2m}q_{2} + \dots h_{m+1,m}q_{m+1}
    \end{equation}

    Un algorithme pour la méthode peut être trouvée en \ref{alg:arnoldi}.

     \begin{algorithm}
    \caption{Itération k dArnoldi}\label{alg:arnoldi}
    \begin{algorithmic}[1]
    \State $A \in \mathbb{K}^{n \times n}$ et $b\in \mathbb{K}^{n}$
    \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$
    
    \For{$j=1,2,\dots k$}
    \State $q_{j+1} = Aq_{j}$

    \For{ $i=1,2,\dots j$}
    \State $h_{ij}= q_{j+1}^{t}q_{i}$
    \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
    \EndFor
    \State $h_{j+1,j}=\norm{q_{j+1}}$
    \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
    \EndFor
    
    \end{algorithmic}
    \end{algorithm}


    
    \chapter{GMRES}

    Un méthode de projetion en $\mathcal{K}_{k}(A,b)$, où les différentes approximations sont prises comme en \ref{eq:init_gmres}, où $Q_{m}$ est le vecteur défini en \ref{eq:init_arnoldi}.

    \begin{equation}\label{eq:init_gmres}
        x = x_{0} + Q_{m}y
    \end{equation}

    Avec \ref{eq:init_gmres} et \ref{eq:init_arnoldi} le résidu devient \ref{eq:final_gmres}, où $x_{0} = 0$, $\beta=\norm{b}$ et $Q_{m+1}^{t}b=(\norm{b} 0 \hspace{0.05in} 0\dots)^{t}$ puisque les colonnes de $Q_{m+1}$ sont des vecteurs orthonormals et $q_{1} = \frac{b}{\norm{b}}$. 

    \begin{align} \label{eq:final_gmres}
    \begin{split}
        r(y) &= \norm{b - Ax}\\ 
        &= \norm{b - A(Q_{m}y)}\\ 
        &= \norm{b-Q_{m+1}H_{m}y} \\
        &= \norm{Q_{m+1}(Q_{m+1}^{t}b-H_{m}y)} \\
        &= \norm{\beta e_{1} - H_{m}y}
    \end{split}
    \end{align}

    Ainsi, $y$ qui apparaît en \ref{eq:init_gmres}, est trouvé comme la solution du problème de minimisation du résidu en \ref{eq:final_gmres}.

    \begin{equation}\label{eq:y_gmres}
        y = min_{y} \norm{\beta e_{1} - H_{m}y}
    \end{equation}

    Une version initiale du GMRES est en \ref{alg:gmres_init}. Les lignes entre 4 et 12 apportent la Méthode d'Arnoldi présentée en \ref{alg:arnoldi}.
    
    \begin{algorithm}
    \caption{GMRES Initial}\label{alg:gmres_init}
    \begin{algorithmic}[1]
    \State $A \in \mathbb{K}^{n \times n}$ et $b\in \mathbb{K}^{n}$
    \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$
    \For{$k=1,2,\dots$}
    \For{$j=1,2,\dots k$}
    \State $q_{j+1} = Aq_{j}$

    \For{ $i=1,2,\dots j$}
    \State $h_{ij}= q_{j+1}^{t}q_{i}$
    \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
    \EndFor
    \State $h_{j+1,j}=\norm{q_{j+1}}$
    \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
    \EndFor
    \State Trouver $y = min_{y} \norm{\beta e_{1} - H_{m}y}$
    \State $x = Q_{k}y$
    \State \textbf{Arrêter} si le résidu est inférieur à la tolérance
    \EndFor
    \end{algorithmic}
    \end{algorithm}

    Cependant, \ref{alg:gmres_init} n'apporte pas une façon efficace de trouver le résidu en chaque itération. Pour le résoudre et trouver aussi une mieux façon de traiter le problème des moindres carrés en \ref{eq:y_gmres}, une transformation est appliquée en $H_{m}$, la transformant dans une matrice triangulaire.

    \section{Transformation de Givens}

    L'opérateur de Givens, $G(i,i+1)$, est une matrice unitaire telle que le vecteur colonne résultant $a = Gb$ a les éléments $a(i) = r \in \mathbb{R}$ et $a(i+1)=0$. Il est une matrice de struture comme en \ref{eq:givens}. Les coefficients $c_{i},s_{i}$ n'apparaissent que dans les lignes $i$ et $i+1$.

    \begin{equation}\label{eq:givens}
    G(i,i+1)=
    \begin{bmatrix}
            1 & & & & & & & \\
             &\ddots & & & & & & \\
             & & 1 & & & & & \\
              & & & c_{i}& s_{i} & & & \\
            & & & -s_{i}& c_{i} & & & \\
            & & & & & 1& & \\
            & & & & & & \ddots& \\
            & & & & & & & 1\\
    \end{bmatrix}
    \end{equation}

    L'opérateur est une façon de transformer les colonnes de $H_{m}$, annulant l'élément dehors la diagonale. Comme un produit d'opérateurs unitaires est encore unitaire, cela nous permet récrire \ref{eq:y_gmres} comme \ref{eq:after_givens}, où $R_{m}$ et $g_{m}$ sont les résultats de l'application des opérateurs de Givens à $H_{m}$ et $\beta e_{1}$.

    \begin{equation}\label{eq:after_givens}
        y = min_{y} \norm{\beta e_{1} - H_{m}y} = min_{y} \norm{g_{m} - R_{m}y}
    \end{equation}

    Il peut être montré que $g_{m}$ contient aussi la valeur du résidu de chaque itération \cite{saad2003iterative}.

    Ainsi, le nouveau problème \ref{eq:after_givens} peut être résolu avec une simple substitution.

    (écrire el nouvel algorithme )

    %Mudar para aproximacao lwo rank 
    \chapter{Matrices Hiérarchiques et méthode ACA}

    \section{Matrices low-rank}
    
    En pratique, les matrices sont de grande taille, de façon que stocker chaque élément n'est pas efficace ou même faisable.  Si $A \in \mathbb{C}^{n\times m}$ a un rang \textit{k} tel que $k \leq m$ et que $k(n+m) < n*m$ (\textit{A} est low-rank), \textit{A} peut être représentée comme un produit entre deux matrices $U \in \mathbb{C}^{n\times k} $ et $V \in \mathbb{C}^{m\times k}$, ce qui est vu en \ref{eq::outer_product}, où $u_{i}, v_{i}$ sont les vecteurs colonnes de \textit{U} et \textit{V}.

    \begin{equation}\label{eq::outer_product}
        A = UV^{H} = \sum_{i=1} ^{k} u_{i} v_{i} ^{*}
    \end{equation}

    Ainsi, en stockant $k(n+m)$ éléments pour représenter $A$, au lieu de $n\times m$. Une matrice \textit{A} qui peut être représentée comme \ref{eq::outer_product} est dite un élément de $\mathbb{C}^{n\times m}_{k}$

    La représentation en \ref{eq::outer_product} facilite aussi des autres opérations possibles avec $A$, comme les produits matrice-vecteur $Ab$ qui sont toujours présents dans les méthodes itératives comme GMRES \cite{bebendorf2008hierarchical} et les différentes normes, comme $\norm{A}_{F}, \norm{A}_{2}$ \cite{bebendorf2008hierarchical}. 

    Cependant, même matrices de 'full rank', c-à-d matrices peuvent être approximées par matrices de rang plus petit. Un théorème \cite{bebendorf2008hierarchical} établit que la plus proche matrice en $\mathbb{C}^{n\times m}_{k}$ d'une matrice en $\mathbb{C}^{n\times m}$ peut être obtenue de la SVD $A = U \Sigma V^{H}$, où $\Sigma$ contient les valeurs singulières $\sigma_{1} \geq \sigma_{2} \dots \sigma_{m} \geq 0$ et $U,V$ sont unitaires.

    Si $ A_{k} $ est l'approximation obtenue en prenant les k premiers éléments de $\Sigma$ (en créant la matrice $ \Sigma_{k} $ ), l'erreur entre $ A $ et $ A_{k} $ est obtenu en \ref{eq:error_lowrank}.

    \begin{equation}\label{eq:error_lowrank}
        \norm{A-A_{k}} = \norm{U\Sigma V^{H} - U^{'} \Sigma_{k} V_{' H}} = \norm{\Sigma - \Sigma_{k}} 
    \end{equation}

    Si la norme spectrale, $\norm{.}_{2} $ est utilisée, l'erreur en \ref{eq:error_lowrank} est donné par $\sigma_{k+1}$. Pour la norme de Frobenius, $\norm{.}_{F}$, l'erreur devient $\sum^{n}_{l=k+1} \sigma^{2}_{l}$.

    Au lieu d'approximer des grandes matrices en une seule fois, c'est mieux penser dans les approximations faites pour leurs blocs. Des blocs qui vient d'une discrétisation d'opérateurs elliptiques  ont aussi la possibilité d'être aproximés par matrices qui décroissent exponentiellement  $S_{k}$, comme en \ref{eq:exp_matrix}.

    \begin{equation}\label{eq:exp_matrix}
        \norm{A-S_{k}}_{2} < q^{k}\norm{A}_{2}
    \end{equation}

    Ainsi, le rang dépend de la précision d'une façon logarithme, et le rang nécessaire pour une certaine $\epsilon$ est \ref{eq:exp_error}.

    \begin{equation}\label{eq:exp_error}
        k(\epsilon) = min\{ k \in \mathbb{N} : \sigma_{k+1} < \epsilon\sigma_{1}\}
    \end{equation}

    \section{Méthode ACA(Adaptative Cross Approximation)}

    Comment montré dans la section antérieure, la méthode SVD donne une approximation de $A$ à partir d'une erreur $\epsilon$, à travers de la relation en \ref{eq:error_lowrank}. Néanmoins, c'est une méthode lourde, où la complexité la rend infaisable pour les grands calculs qui peuvent apparaître normalement. L'ACA arrive comme une alternative plus efficace pour les problemes où le noyau est asymptotiquement lisse pour au moins une variable. Il faut mentionner que le noyau lui même n'est pas nécessaire, juste l'information qu'il appartient à ce groupe de fonctions.
    
    
    L'algorithme pour la méthode est en \ref{alg:aca_method}, où $a_{ij}$ sont les éléments d'une matrice $A \in \mathbb{R}^{n\times m}$. L'objectif est d'approximer la matrice $A$ pour $A=S_{k} + R_{k}$, $S_{k} = \sum_{l=1}^{k} u_{l}v_{l}^{t}$ et $R_{k}$ est le résidu.


    \begin{algorithm}
    \caption{Méthode ACA}\label{alg:aca_method}
    \begin{algorithmic}[1]
    \State $k=1$ et $\mathbf{Z} = \emptyset $
    \Repeat
    \State Trouver $i_{k}$
    \State $\hat{v}_{k} = a_{i_{k},1:m} $
    \For{$l=1,\dots , k-1$}
    \State $\hat{v}_{k} = \hat{v}_{k} - (u_{l})_{i_{k}}v_{l} $
    \EndFor
    \State $Z = Z \bigcup \{ i_{k} \} $

    \If{$\hat{v}_{k}$ ne disparaît pas}
    \State $j_{k} = argmax_{j}|(\hat{v}_{k})_{j}|$ ; $v_{k} = (\hat{v}_{k})^{-1}_{j_{k}} \hat{v}_{k}$
    \State $u_{k}=a_{1:n,j_{k}}$

    \For{$l=1,\dots,k-1$}
    \State $u_{k}=u_{k} - (v_{l})_{j_{k}}u_{l}$
    \EndFor
    \State $k=k+1$
    
    \EndIf

    
    \Until{$\norm{u_{k}}\norm{v_{k}} \leq \epsilon$}
    
    \end{algorithmic}
    \end{algorithm}
    
    Soit $I,J \in \mathbb{N}$ l'ensemble des index d'une matrice quelconque et $\mathbf{T}_{I \times J}$ l'arbre que contient une partition admissible \textit{P} de $ I \times J$ dans ses feuilles, $\mathfrak{L}(\mathbf{T}_{I \times J})$. L'ensemble des matrices hiérarchiques en $\mathbf{T}_{I \times J}$ rang k pour chaque bloc $A_{b}$ est définit en \ref{eq:matrix_hier}.

    \begin{equation}\label{eq:matrix_hier}
        \mathfrak{H}(\mathbf{T}_{I \times J},k) = \left\{  A\in \mathbb{C}^{I\times J} : rankA_{b} \leq k, \forall b \in P \right\}
    \end{equation}
    
    \newpage

    \addcontentsline{toc}{chapter}{\bibname}


    \bibliography{bibliografia}
    \bibliographystyle{plain}
    \nocite{*}
    
    
    
\end{document}
