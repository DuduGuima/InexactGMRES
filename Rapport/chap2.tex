
    A projection in $\mathcal{K}_{k}(A,b)$, where the different approximations are taken as in \ref{eq:init_gmres}, where $Q_{m}$ is the vector in \ref{eq:init_arnoldi}.
    \begin{equation}\label{eq:init_gmres}
        x = x_{0} + Q_{m}y
    \end{equation}

    With \ref{eq:init_gmres} and \ref{eq:init_arnoldi} the residue becomes \ref{eq:final_gmres}, where $x_{0} = 0$, $\beta=\norm{b}$ and $Q_{m+1}^{t}b=(\norm{b} 0 \hspace{0.05in} 0\dots)^{t}$ since the columns of $Q_{m+1}$ are orthonormal vectors and $q_{1} = \frac{b}{\norm{b}}$. 

    \begin{align} \label{eq:final_gmres}
    \begin{split}
        r(y) &= \norm{b - Ax}\\ 
        &= \norm{b - A(Q_{m}y)}\\ 
        &= \norm{b-Q_{m+1}H_{m}y} \\
        &= \norm{Q_{m+1}(Q_{m+1}^{t}b-H_{m}y)} \\
        &= \norm{\beta e_{1} - H_{m}y}
    \end{split}
    \end{align}

    
    Thus, $y$ which appears in \ref{eq:init_gmres}, is found as the solution of the residual's minimisation problem  in \ref{eq:final_gmres}.

    \begin{equation}\label{eq:y_gmres}
        y = min_{y} \norm{\beta e_{1} - H_{m}y}
    \end{equation}

    An initial version of the GMRES is in \ref{alg:gmres_init}. The lines 4 to 12 bring the Arnoldi's Method presented in \ref{alg:arnoldi}.
    
    \begin{algorithm}
    \caption{Initial GMRES}\label{alg:gmres_init}
    \begin{algorithmic}[1]
    \State $A \in \mathbb{K}^{n \times n}$ and $b\in \mathbb{K}^{n}$
    \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$
    \For{$k=1,2,\dots$}
    \For{$j=1,2,\dots k$}
    \State $q_{j+1} = Aq_{j}$

    \For{ $i=1,2,\dots j$}
    \State $h_{ij}= q_{j+1}^{t}q_{i}$
    \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
    \EndFor
    \State $h_{j+1,j}=\norm{q_{j+1}}$
    \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
    \EndFor
    \State Find $y = min_{y} \norm{\beta e_{1} - H_{m}y}$
    \State $x = Q_{k}y$
    \State \textbf{Stop} if the residual is smaller than the tolerance 
    \EndFor
    \end{algorithmic}
    \end{algorithm}

    However, \ref{alg:gmres_init} doesn't bring an efficient way of finding the residual in each iteration. To solve this problem and also to find a more efficient way of solving the least squares problem in \ref{eq:y_gmres}, a transformation is applied to $H_{m}$, turning it into a triangular matrix.

    \section{Givens's Rotation}

    Givens's operator, $G(i,i+1)$, is an unitary matrix such that the column vector $a = Gb$ has the elements $a(i) = r \in \mathbb{R}$ and $a(i+1)=0$. It has a structure as in \ref{eq:givens}. The coefficients $c_{i},s_{i}$ only appear in the rows $i$ et $i+1$.

    \begin{equation}\label{eq:givens}
    G(i,i+1)=
    \begin{bmatrix}
            1 & & & & & & & \\
             &\ddots & & & & & & \\
             & & 1 & & & & & \\
              & & & c_{i}& s_{i} & & & \\
            & & & -s_{i}& c_{i} & & & \\
            & & & & & 1& & \\
            & & & & & & \ddots& \\
            & & & & & & & 1\\
    \end{bmatrix}
    \end{equation}
    This operator offers a way to transform the columns in $H_{m}$, \textit{zeroing} the elements outside the main diagonal. Since a product of unitary operators is still unitary, \ref{eq:y_gmres} can be written as \ref{eq:after_givens}, where $R_{m}$ and $g_{m}$ are the results from the application of multiple Givens's operators to $H_{m}$ and $\beta e_{1}$.

    \begin{equation}\label{eq:after_givens}
        y = min_{y} \norm{\beta e_{1} - H_{m}y} = min_{y} \norm{g_{m} - R_{m}y}
    \end{equation}

    It can be shown that $g_{m}$ contains the residual of each iteration \cite{saad2003iterative}.

    Thus, the new problem \ref{eq:after_givens} can be solved with a simple backwards substitution.

    (Ã©crire le nouvel algorithme )

    \section{Inexact GMRES}

    The heaviest part in the code is in the matrix-vector product \ref{alg:arnoldi}, line 4. Therefore, one approach to accelerate the iterations involves an approximation of $Aq $, instead of using the exact answer, as shown in \ref{eq:aprox_Aq}.

    \begin{equation}\label{eq:aprox_Aq}
        \mathcal{A}q = (A + E)q
    \end{equation}
    
    Where \textit{E} in \ref{eq:aprox_Aq} is a \textit{pertubation matrix} that changes with each iteration and will be written as $E_{k}$ for iteration k.

    When the matrix-vector product isn't exact, the left side of \ref{eq:init_arnoldi} must be changed by \ref{eq:new_projection}.

    % \begin{equation}\label{eq:new_projection}
    %     [(A + E_{1})q_{1}, (A + E_{2})q_{2},\dots, (A + E_{k})q_{k}] = Q_{k+1}H_{k}
    % \end{equation}

    \begin{align} \label{eq:new_projection}
    \begin{split}
        [(A + E_{1})q_{1}, (A + E_{2})q_{2},\dots, (A + E_{k})q_{k}] &= Q_{k+1}H_{k}\\ 
        (A + \mathcal{E}_{k})Q_{k} &= Q_{k+1}H_{k}, \hspace{0.1in} \mathcal{E}_{k} = \sum_{i=1}^{k}E_{i}q_{i}q_{i}^{t}\\
        \mathcal{A}Q_{k} &= Q_{k+1}H_{k}
    \end{split}
    \end{align}

    Now the subspace spawn by the vectors of $Q_{k}$ is not a Krylov's subspace, bbut these are still orthonormal.
    
    The expression \ref{eq:new_projection} also shows that $Q_{k}$ becomes a basis for a Krylov's subspace, $\mathcal{K}_{k}(A+\mathcal{E}_{k},b)$, made by a big pertubation in $A$, that gets updated in each iteration.
    
    Using $r_{k}$, the exact residue, and $\tilde{r}_{k}$, the residue from the Inexact GMRES's k-th iteration, it can be shown \cite{simoncini2003theory} that if $E_{k}$ satisfies the relation \ref{eq:res_Hinexact}, then the two residues satisfy \ref{eq:res_igmres}.


    \begin{equation}\label{eq:res_Hinexact}
        \norm{E_{k}} \leq \frac{\sigma_{k} \left( \hat{H}_{k} \right) }{k} \frac{1}{\norm{\tilde{r}_{k}}} \epsilon
    \end{equation}

    \begin{equation}\label{eq:res_igmres}
        \norm{r_{k}-\tilde{r}_{k}} \leq \epsilon
    \end{equation}

    In \ref{eq:res_Hinexact}, $\hat{H}_{k}$ is the upper triangular matrix got after Givens's rotations in $H_{k}$ and $\sigma_{k}$ is the k-th singular value of a given matrix.