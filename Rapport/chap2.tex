
    A projection in $\mathcal{K}_{k}(A,b)$, where the different approximations are taken as in \ref{eq:init_gmres}, where $Q_{m}$ is the vector in \ref{eq:init_arnoldi}.
    \begin{equation}\label{eq:init_gmres}
        x = x_{0} + Q_{m}y
    \end{equation}

    With \ref{eq:init_gmres} and \ref{eq:init_arnoldi} the residue becomes \ref{eq:final_gmres}, where $x_{0} = 0$, $\beta=\norm{b}$ and $Q_{m+1}^{t}b=(\norm{b} 0 \hspace{0.05in} 0\dots)^{t}$ since the columns of $Q_{m+1}$ are orthonormal vectors and $q_{1} = \frac{b}{\norm{b}}$. 

    \begin{align} \label{eq:final_gmres}
    \begin{split}
        r(y) &= \norm{b - Ax}\\ 
        &= \norm{b - A(Q_{m}y)}\\ 
        &= \norm{b-Q_{m+1}H_{m}y} \\
        &= \norm{Q_{m+1}(Q_{m+1}^{t}b-H_{m}y)} \\
        &= \norm{\beta e_{1} - H_{m}y}
    \end{split}
    \end{align}

    
    Thus, $y$ which appears in \ref{eq:init_gmres}, is found as the solution of the residual's minimisation problem  in \ref{eq:final_gmres}.

    \begin{equation}\label{eq:y_gmres}
        y = min_{y} \norm{\beta e_{1} - H_{m}y}
    \end{equation}

    An initial version of the GMRES is in \ref{alg:gmres_init}. The lines 4 to 12 bring the Arnoldi's Method presented in \ref{alg:arnoldi}.
    
    \begin{algorithm}
    \caption{Initial GMRES}\label{alg:gmres_init}
    \begin{algorithmic}[1]
    \State $A \in \mathbb{K}^{n \times n}$ and $b\in \mathbb{K}^{n}$
    \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$
    \For{$k=1,2,\dots$}
    \For{$j=1,2,\dots k$}
    \State $q_{j+1} = Aq_{j}$

    \For{ $i=1,2,\dots j$}
    \State $h_{ij}= q_{j+1}^{t}q_{i}$
    \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
    \EndFor
    \State $h_{j+1,j}=\norm{q_{j+1}}$
    \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
    \EndFor
    \State Find $y = min_{y} \norm{\beta e_{1} - H_{m}y}$
    \State $x = Q_{k}y$
    \State \textbf{Stop} if the residual is smaller than the tolerance 
    \EndFor
    \end{algorithmic}
    \end{algorithm}

    However, \ref{alg:gmres_init} doesn't bring an efficient way of finding the residual in each iteration. To solve this problem and also to find a more efficient way of solving the least squares problem in \ref{eq:y_gmres}, a transformation is applied to $H_{m}$, turning it into a triangular matrix.

    \section{Givens's Rotation}

    Givens's operator, $G(i,i+1)$, is an unitary matrix such that the column vector $a = Gb$ has the elements $a(i) = r \in \mathbb{R}$ and $a(i+1)=0$. It has a structure as in \ref{eq:givens}. The coefficients $c_{i},s_{i}$ only appear in the rows $i$ et $i+1$.

    \begin{equation}\label{eq:givens}
    G(i,i+1)=
    \begin{bmatrix}
            1 & & & & & & & \\
             &\ddots & & & & & & \\
             & & 1 & & & & & \\
              & & & c_{i}& s_{i} & & & \\
            & & & -s_{i}& c_{i} & & & \\
            & & & & & 1& & \\
            & & & & & & \ddots& \\
            & & & & & & & 1\\
    \end{bmatrix}
    \end{equation}
    This operator offers a way to transform the columns in $H_{m}$, \textit{zeroing} the elements outside the main diagonal. Since a product of unitary operators is still unitary, \ref{eq:y_gmres} can be written as \ref{eq:after_givens}, where $R_{m}$ and $g_{m}$ are the results from the application of multiple Givens's operators to $H_{m}$ and $\beta e_{1}$.

    \begin{equation}\label{eq:after_givens}
        y = min_{y} \norm{\beta e_{1} - H_{m}y} = min_{y} \norm{g_{m} - R_{m}y}
    \end{equation}

    It can be shown that $g_{m}$ contains the residual of each iteration \cite{saad2003iterative}.

    Thus, the new problem \ref{eq:after_givens} can be solved with a simple backwards substitution.

    (Ã©crire le nouvel algorithme )

    \section{Inexact GMRES}

    The heaviest part in the code is in the matrix-vector product \ref{alg:arnoldi}, line 4. Therefore, one approach to accelerate the iterations involves an approximation of $Aq $, instead of using the exact answer, as shown in \ref{eq:aprox_Aq}.

    \begin{equation}\label{eq:aprox_Aq}
        \mathcal{A}q = (A + E)q
    \end{equation}
    
    Where \textit{E} in \ref{eq:aprox_Aq} is a \textit{pertubation matrix} that changes with each iteration and will be written as $E_{k}$ for iteration k.

    When the inexact matrix-vector product is the one being made, the left side of \ref{eq:init_arnoldi} must be changed by \ref{eq:new_projection}.


    \begin{align} \label{eq:new_projection}
    \begin{split}
        [(A + E_{1})q_{1}, (A + E_{2})q_{2},\dots, (A + E_{k})q_{k}] &= Q_{k+1}H_{k}\\ 
        (A + \mathcal{E}_{k})Q_{k} &= Q_{k+1}H_{k}, \hspace{0.1in} \mathcal{E}_{k} = \sum_{i=1}^{k}E_{i}q_{i}q_{i}^{t}\\
        \mathcal{A}Q_{k} &= W_{k}
    \end{split}
    \end{align}
    Where $W_{m} = Q_{m+1}H_{m}$ from this point foward.

    Now the subspace spawn by the vectors of $Q_{k}$ is not the Krylov's subspace $\mathcal{K}_{k}(A,b)$ , but these are still orthonormal. The expression \ref{eq:new_projection} also shows that $Q_{k}$ becomes a basis for a new Krylov's subspace, $\mathcal{K}_{k}(A+\mathcal{E}_{k},b)$, made by a big pertubation in $A$, that gets updated in each iteration.
    
    A new distinction should also be made between the two types of residues appearing in the process: $r_{k}$, the exact residue of an iteration, and $\tilde{r}_{k}$, the one that will really be calculated. A detailed definition for both and a measure of how distant they are is in \ref{eq:res_relation}.

    \begin{align}\label{eq:res_relation}
    \begin{split}
        r_{k} &= r_{0} - AQ_{k}y_{k}\\ 
        &= r_{0} - (Q_{k+1}H_{k} - [E_{1}q_{1},\dots , E_{k}q_{k}])y_{k}\\
        &= \tilde{r}_{k} +[E_{1}q_{1},\dots , E_{k}q_{k}]y_{k}\\
        \rightarrow \delta_{k} &= \norm{r_{k} - \tilde{r}_{k}}  = \norm{[E_{1}q_{1},\dots , E_{k}q_{k}]y_{k}}
    \end{split}
    \end{align}

    Considering $y_{k} = [\eta_{1}^{(k)} \dots \eta_{n}^{(k)} ] $, upper index to clarify the iteration, an upper bound for $\delta_{k}$ can be found \cite{simoncini2003theory}, as shown in \ref{eq:borne_delta}

    \begin{equation}\label{eq:borne_delta}
        \delta_{k} = \norm{r_{k} - \tilde{r}_{k}} \leq \sum_{i=1}^{k} \norm{E_{i}} \norm{\eta_{i}^{(k)}}
    \end{equation}
    
    \ref{eq:borne_delta} tells us that in order to keep both residues close, either the pertubation of $A$, somewhat measured by $\norm{E_{i}}$, or the elements of $y_{i}$ should be kept small. Since we expect to use more \textit{relaxed} approximations of $A$ as the iterations go on, a greater tolerance in $E_{k}$ could be compensated with a sufficiently small $y_{k}$.

    
    The problem is $y_{k}$ is only found after the construction of $E_{k}$, so an upper bound must be also found for its value.
    It can be shown, lemma 5.1 in \cite{simoncini2003theory}, that for $i = 1,2,\dots,k$, where k is the iteration number, the bound is given by \ref{eq:bound_yinexact}.

    \begin{equation}\label{eq:bound_yinexact}
        \norm{\eta_{i}^{(k)}} \leq \frac{1}{\sigma_{k}(H_{k})} \norm{\tilde{r}_{i-1}}
    \end{equation}

    Putting \ref{eq:bound_yinexact} in \ref{eq:borne_delta} gives the results \ref{eq:boundE_intermediate}. Setting $\delta_{k} \leq \epsilon$ and determining a bound for each $\norm{E_{i}}$ gets us \ref{eq:bound_E}.


    \begin{equation}\label{eq:boundE_intermediate}
        \delta_{k} \leq \sum_{i=1}^{k} \frac{\norm{E_{i}}}{\sigma_{k}(H_{k})}\norm{\tilde{r}_{i-1}}
    \end{equation}

    \begin{equation}\label{eq:bound_E}
        \norm{E_{i}} \leq \frac{\sigma_{k}(H_{k})\epsilon}{k\norm{\tilde{r}_{i-1}}}
    \end{equation}

    Since $H_{k}$ is also one of the matrices being constructed throuhout the method, a workaround is necessary to apply find these bounds in a pratical situation. Either using an estimation of $\sigma_{k}(H_{k})$ with the singular values of $A$ or grouping all uncalculated terms in a $\ell_{k}$ that will be estimated empirically \cite{simoncini2003theory}, obtaining \ref{eq:boundE_final}.

    \begin{equation}\label{eq:boundE_final}
        \norm{E_{i}} \leq \ell_{k} \frac{1}{\norm{\tilde{r}_{i-1}}} \epsilon
    \end{equation}