\section{Iterative Methods and motivation}

%Iterative methods appear as an alternative to exact solution methods, where the true solution is not desired and a good approximation is enough.

%Iterative methods appear as an alternative to direct solution methods, where the true/exact solution to a problem is only desired up to a certaion tolerance, usually used when the exact methods need heavy computing power and/or are inefficient in the current context.
Iterative methods appear as an alternative to direct solution methods, where the direct solution to a problem usually scales with $\mathcal{O}(m^{3})$ complexity, where $m$ is the dimension of the input matrix. Since larger matrices are usually employed in practise, the direct algorithms become inefficient and a more reliable approach is desired.


The idea of the iterable methods is to find, after a certain number of iterations, a sequence ${x_{k}}$ that converges to $x$, the exact solution of the problem \ref{eq:suite}, all while making the large-scale computations faster, i.e. obtaining a complexity smaller than $\mathcal{O}(m^{3})$, and keeping a maximum tolerance between the iterable solution and the exact one.


\begin{equation}\label{eq:suite}
    x = \lim_{k \to \infty} x_{k}
\end{equation}


The method stops after $k$ iterations, where $x_{k}$ is the fist element of the sequence to satisfy the condition \ref{eq:ch1_it}.


\begin{equation}\label{eq:ch1_it}
    \frac{||x_{k} - x||}{||x||} \leq \epsilon
\end{equation}
Where $\epsilon$ is a tolerance defined by who is applying the algorithm.

Usually $x$ isn't known, so \ref{eq:ch1_it} gets modified for \ref{eq:residual}, where $A$ is the system's matrix and $b$ is the RHS(right hand side).

\begin{equation}\label{eq:residual}
    \frac{||Ax_{k} - b||}{||b||} \leq \epsilon
\end{equation}

(I'm thinking about removing this part completely)

The first iterative methods used a decomposition of $A$ as a combination of two matrices \ref{eq:A-comb}, where $A_{1}$ isn't singular, and each iteration is defined as \ref{eq:A_it}.

\begin{equation}\label{eq:A-comb}
    A = A_{1} - A_{2}
\end{equation}

\begin{equation}\label{eq:A_it}
    A_{1} x_{k+1} = b + A_{2}x_{k}
\end{equation}

With a substitution of the others $x_{k}$, \ref{eq:A_it} gives \ref{eq:it_fin}, which converges for every initial solution if and only lff $\rho(A_{2}A_{1}^{-1}) < 1$, or $\rho(X)$ is the spectral radius of X \cite{bonnet}.

\begin{equation}\label{eq:it_fin}
    x_{k+1} = A_{1}^{-1}(b + A_{2}x_{k}) = A_{1}^{-1}(b + A_{2}A_{1}^{-1}(b + A_{2}x_{k-1}))... = A_{1}^{-1} \left[ \sum_{i=0}^{k} (A_{2}A_{1}^{-1})^{i}b\right]
\end{equation}
If $A_{1} = I$ and $A_{2} = I - A$ in \ref{eq:A-comb}, the sequence found in \ref{eq:it_fin} is: $x_{1} = b$,$x_{2} = 2b- Ab$, $x_{3} = 3b-3Ab+A^{2}b$ , $\dots$

Even if the condition $\rho(A-I) \leq 1$ is strong \cite{bonnet}, it shows that one approximation $x_{k}$ could be represented as \ref{eq:xkry}.

\begin{equation}\label{eq:xkry}
    x_{k} \in span(b,Ab,A^{2}b,...,A^{k-1}b)
\end{equation}

\section{Krylov's Subspace}
Be $A \in \mathbb{K}^{n \times n}$ a matrix and $b\in \mathbb{K}^{n}$. To each $k\leq n$ the Krylov's Subspace $\mathcal{K}_{k}=\mathcal{K}_{k}(A,b)$ associated to A,b is defined as \ref{eq:krylov}.

\begin{equation}\label{eq:krylov}
    \mathcal{K}_{k}(A,b) = span(b,Ab,A^{2}b,\dots , A^{k-1}b)
\end{equation}

These Subspaces also have the following property: $k<l \to \mathcal{K}^{k} \subset \mathcal{K}^{l}$ \cite{bonnet}.

The subspace $\mathcal{K}_{k}(A,b)$ is also the subspace of all the vectors from $\mathbb{R}^{m}$ which could be written as $x=p(A)b$, where $p(A)$ is a polynom of degree less than $k-1$ which $p(0)=1$.

The problem with using ${A^{k}b}, k \in {0,1,2,\dots}$ as a base comes from the fact that successive products of $A$ make vectors that are \textit{approximately colinear}, since those are really close of the eigenvector with the largest eigenvalue of $A$.

\section{Arnoldi's Method}

With the task of obtaining an orthonormal basis to $\mathcal{K}_{k}(A,b)$, the method searches for a unitary matrix $Q$ for which the expression \ref{eq:init_arnoldi} is valid. $H_{k}={h_{ij}}$ is an Hessenberg's matrix.


\begin{equation} \label{eq:init_arnoldi}
    AQ_{k} = Q_{k+1}H_{k}
\end{equation}

For each column-vector of $Q$, $q_{i}$, \ref{eq:init_arnoldi} could be written as \ref{eq:final_arnoldi}, where the representation of $\mathcal{K}_{k}(A,b)$ with an orthonormal basis becomes more evident. In a pratical application, $Q$ is initialized with $q_{1} = \frac{b}{||b||}$.

\begin{equation}\label{eq:final_arnoldi}
    Aq_{m} = h_{1m}q_{1} + h_{2m}q_{2} + \dots h_{m+1,m}q_{m+1}
\end{equation}

An algorithm for the method can be found in \ref{alg:arnoldi}.

\begin{algorithm}
    \caption{Arnoldi's iteration}\label{alg:arnoldi}
    \begin{algorithmic}[1]
        \State $A \in \mathbb{K}^{n \times n}$ et $b\in \mathbb{K}^{n}$
        \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$

        \For{$j=1,2,\dots k$}
        \State $q_{j+1} = Aq_{j}$

        \For{ $i=1,2,\dots j$}
        \State $h_{ij}= q_{j+1}^{t}q_{i}$
        \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
        \EndFor
        \State $h_{j+1,j}=\norm{q_{j+1}}$
        \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
        \EndFor

    \end{algorithmic}
\end{algorithm}