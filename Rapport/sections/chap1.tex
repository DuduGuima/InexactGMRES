\section{Iterative Methods and motivation}

We consider the solution of a linear system, with $A \in \mathbb{C}^{n \times n}$ an invertible matrix, $b \in \mathbb{C}^{n}$ our right-hand side and $x\in \mathbb{C}^{n}$ a vector of unknowns:

\begin{equation}\label{eq:linear_systinit}
    Ax=b
\end{equation}


A system that can be solved by direct methods, like \textit{Gaussian elimination} and \textit{LU decomposition}. The problem with such approaches comes with their complexity: direct methods usually have $\mathcal{O}(m^{3})$ complexity \cite{trefethen1998numerical}, where $m$ is the dimension of the input matrix. Since we apply larger matrices in practice, to achieve high resolution in the solution of a \acrshort{pde}, the direct algorithms become too slow, and we require a more efficient approach.


These iterable methods find, after a certain number of iterations, a sequence ${x_{k}}$ that converges to $x$, the exact solution of the problem \ref{eq:linear_systinit}. This should be done while making the large-scale computations faster, i.e., obtaining a complexity smaller than $\mathcal{O}(m^{3})$, and keeping a maximum tolerance between the iterable solution and the exact one.


\begin{equation}\label{eq:suite}
    x = \lim_{k \to \infty} x_{k}
\end{equation}


The method stops after $k$ iterations, where $x_{k}$ is the fist element of the sequence to satisfy the condition \ref{eq:residual}.

\begin{equation}\label{eq:residual}
    \frac{||Ax_{k} - b||}{||b||} \leq \epsilon
\end{equation}

We define $\epsilon$ as the tolerance given to the algorithm.

To achieve a smaller complexity than $\mathcal{O}(m^{3})$, Iterative Methods employ matrix vector products, complexity $\mathcal{O}(m^{2})$, instead of the product between matrices found in direct methods. So, considering an Iterative Method finds a solution in $k$ steps, its complexity would be $\mathcal{O}(km^{2})$.

Therefore, if $k<<m$, i.e., the number of iterations is sufficiently smaller than $m$, the iterative methods become more efficient than the direct ones.



The method we employ in our problems is the \acrfull{gmres}, explained later in the report. Its main idea involves the projection of a high dimensional problem, as large as $A$ in \ref{eq:residual}, in a lower dimensional \textit{Krylov Space}:

\begin{equation}\label{eq:xkry}
    x_{k} \in \spn(b,Ab,A^{2}b,...,A^{k-1}b)
\end{equation}

Explained in more detail below.

\section{Krylov subspace}
Let $A \in \mathbb{C}^{n \times n}$ a matrix and $b\in \mathbb{C}^{n}$. For each $k\leq n$ the Krylov subspace $\mathcal{K}_{k}=\mathcal{K}_{k}(A,b)$ associated to A, b is defined as \ref{eq:krylov}.

\begin{equation}\label{eq:krylov}
    \mathcal{K}_{k}(A,b) = \spn(b,Ab,A^{2}b,\dots , A^{k-1}b)
\end{equation}

These subspaces also have the following property: $k<l \to \mathcal{K}^{k} \subset \mathcal{K}^{l}$ \cite{bonnet}.

The subspace $\mathcal{K}_{k}(A,b)$ is also the subspace of all the vectors from $\mathbb{R}^{m}$ which could be written as $x=p(A)b$, where $p(A)$ is a polynomial of degree less than $k-1$ which $p(0)=1$.

The problem with using ${A^{k}b}, k \in {0,1,2,\dots}$ as a base comes from the fact that successive products of $A$ make vectors that are \textit{approximately collinear}, since those are really close of the eigenvector with the largest eigenvalue of $A$.

Such problem then requires finding a new base for this space, one that does not suffer from the power iteration of $A$, and it is the subject of the next section.

\section{Arnoldi's Method}


Arnoldi's Method is an orthogonal projection method used to find an orthonormal basis ${q_{1}, \dots, q_{k}}$ to $\mathcal{K}_{k}(A,b)$.

An algorithm for the method can be found in \ref{alg:arnoldi}.


\begin{algorithm}
    \caption{Arnoldi's iteration}\label{alg:arnoldi}
    \begin{algorithmic}[1]
        \State $A \in \mathbb{K}^{n \times n}$ et $b\in \mathbb{K}^{n}$
        \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$

        \For{$j=1,2,\dots k$}
        \State $q_{j+1} = Aq_{j}$

        \For{ $i=1,2,\dots j$}
        \State $h_{ij}= q_{j+1}^{t}q_{i}$
        \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
        \EndFor
        \State $h_{j+1,j}=\norm{q_{j+1}}$
        \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
        \EndFor

    \end{algorithmic}
\end{algorithm}


As we can see, at each step in \ref{alg:arnoldi}, the previous vector $q_{j}$ is multiplied by $A$ and then orthonormalized in relation to all previous $q_{i}$'s with a Gram-Schmidt procedure. If $q_{j+1}$ ever vanishes during the inner loop between lines 5 and 8, the algorithm stops.

What is left is to show the $q_{i}$ generated by \ref{alg:arnoldi} form an orthonormal basis for $\mathcal{K}_{k}(A,b)$.

\begin{proposition}\label{prop:arnoldi_basis}
    Assume that
    \ref{alg:arnoldi} does not stop before the k-th step. Then the vectors $q_{1}, q_{2},
        \dots, q_{k}$ form an orthonormal basis of the Krylov subspace $\mathcal{K}_{k}(A,b)$

    $$
        \mathcal{K}_{k}(A,b) = \spn \{b, Ab, A^{2}b, \dots, A^{k-1}b\}
    $$
\end{proposition}

\begin{proof}\label{proof:arnoldi}
    By construction $q_{j}$, $j = 1,2,\dots, k$ are orthonormal. To show they span $\mathcal{K}_{k}(A,b)$ we prove $q_{j}$ has the form $p_{j-1}(A)b$, where $p_{j}(A)$ is a polynomial of degree $j-1$ in A.
    Using induction the result is true for $j=1$ since $q_{1} = b$. We assume the result is true for all integers $\leq j$ and consider $q_{j+1}$. Using the definition of $q_{j+1}$ in \ref{alg:arnoldi} we have:

    \begin{equation}
        h_{j+1, j}q_{j+1} = Aq_{j} - \sum_{i=1}^{j} h_{ij}q_{i} = Ap_{j-1}(A)b - \sum_{i=1}^{j} h_{ij}p_{i-1}(A)b
    \end{equation}

    Since, by the induction step above, $q_{i} = p_{i-1}(A)b$.

    This shows $q_{j+1}$ can be written as $p_{j}(A)b$ and completes the proof.
\end{proof}

We also make note of the fact $q_{1} = \frac{b}{||b||}$.

Next we prove the relation between $A$ and the Hessenberg's matrix defined by \ref{alg:arnoldi}.

\begin{proposition}

    Denote by $Q$ the $n \times k$ matrix with column vectors $q_{1}, \dots, q_{k}$ found in \ref{alg:arnoldi} and $H_{k}$ the $(k + 1)\times k$ Hessenberg matrix whose nonzero entries $h_{ij}$ are given just as in \ref{alg:arnoldi}. Then the following relation holds \ref{eq:init_arnoldi}.

    \begin{equation} \label{eq:init_arnoldi}
        AQ_{k} = Q_{k+1}H_{k}
    \end{equation}
\end{proposition}

\begin{proof}
    For each column-vector of $Q$, $q_{i}$, \ref{eq:init_arnoldi} could be written as \ref{eq:final_arnoldi}, where the representation of $\mathcal{K}_{k}(A,b)$ with an orthonormal basis becomes more evident.

    \begin{equation}\label{eq:final_arnoldi}
        Aq_{m} = h_{1m}q_{1} + h_{2m}q_{2} + \dots h_{m+1,m}q_{m+1}
    \end{equation}

    This relation can be directly seen in \ref{alg:arnoldi} by using line 10 and the inner loop between lines 5 and 8:

    \begin{align}
        \begin{split}
            q_{m+1}h_{m+1,m} & = Aq_{m} - \sum_{i=1}^{m} h_{im}q_{i}\\
            Aq_{m} & = \sum_{i=1}^{m+1} h_{im}q_{i}
        \end{split}
    \end{align}

\end{proof}

This method can also be used to find one(or some) of the eigenvalues of $A$, through the \textit{Rayleigh-Ritz method} \cite{trefethen1998numerical}.

By \ref{eq:init_arnoldi}, $H_{k}$ is a Hessenberg Matrix of dimensions $k+1 \times k$, usually a modest size, with its eigenvalues can be computed more efficiently. These, known as \textit{Ritz eigenvalues}, typically converge to the largest eigenvalues of $A$.
