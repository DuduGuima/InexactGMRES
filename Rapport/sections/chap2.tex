
As mentioned above, GMRES is the iterative method chosen to solve the linear system in \ref{eq:linear_systinit}. This will mainly be done by approximating, at each iteration $k$, the solution $x$ to an element in the lower dimensional Krylov subspace $\mathcal{K}_{k}(A,b)$, $A$ being the matrix associated to the linear system and $b$ its right-hand side.

Defining the residual of iteration $k$ as:

\begin{equation}
    r_{k} = \norm{b - Ax_{k}}
\end{equation}

Our $x_{k}$, the $k-th$ approximation to $x$, if chosen as the solution of the minimization problem of $r_{k}$. Since we make the projections in $\mathcal{K}_{k}(A,b)$, $x_{k} = K_{k}y_{k}$, our solution is given by the problem:

\begin{equation}
    \min_{x \in \mathcal{K}_{k}(A,b)} \norm{b - Ax} = \min_{y \in \mathbb{C}^{n}} \norm{b - AK_{k}y}
\end{equation}

Where $K_{k}$ is the Krylov matrix with columns equal to the vectors that span the Krylov subspace of the current iteration:

\begin{equation}
    K_{k} = \left[b, Ab, A^{2}b, \dots, A^{k}b \right]
\end{equation}

But as said in the previous chapters,${b,Ab, \dots, A^{k}b}$ has approximately collinear vectors, and we choose the base from the Arnoldi's Method instead.

Then, we take a projection in $\mathcal{K}_{k}(A,b)$, where we take the different approximations as in \ref{eq:init_gmres}, where $Q_{m}$ is the vector in \ref{eq:init_arnoldi}.

Using the projections $Q_{k}$ from \ref{eq:init_arnoldi}:

\begin{equation}\label{eq:init_gmres}
    x_{k} = x_{0} + Q_{k}y_{k}
\end{equation}

With \ref{eq:init_gmres} and \ref{eq:init_arnoldi} the residual becomes \ref{eq:final_gmres}, where $x_{0} = 0$, $\beta=\norm{b}$ and $Q_{k+1}^{t}b=(\norm{b} 0 \hspace{0.05in} 0\dots)^{t}$ since the columns of $Q_{k+1}$ are orthonormal vectors and $q_{1} = \frac{b}{\norm{b}}$.

\begin{align} \label{eq:final_gmres}
    \begin{split}
        r_{k} &= \norm{b - Ax_{k}}\\
        &= \norm{b - A(Q_{k}y_{k})}\\
        &= \norm{b-Q_{k+1}H_{k}y_{k}} \\
        &= \norm{Q_{k+1}(Q_{k+1}^{t}b-H_{k}y_{k})} \\
        &= \norm{\beta e_{1} - H_{k}y_{k}}
    \end{split}
\end{align}


Thus, $y_{k}$ which appears in \ref{eq:init_gmres}, is found as the solution of the residual's minimization problem  in \ref{eq:final_gmres}:

\begin{equation}\label{eq:y_gmres}
    y_{k} = \argmin_{y \in \mathbb{C}^{k}} \norm{\beta e_{1} - H_{k}y}
\end{equation}

An initial version of the GMRES is in \ref{alg:gmres_init}. The lines 4 to 12 contain the Arnoldi's Method presented in \ref{alg:arnoldi}.

\begin{algorithm}
    \caption{Initial GMRES}\label{alg:gmres_init}
    \begin{algorithmic}[1]
        \State $A \in \mathbb{K}^{n \times n}$ and $b\in \mathbb{K}^{n}$
        \State $x=0, \beta=\norm{b},q_{1}=\frac{b}{\beta}$
        \For{$k=1,2,\dots$}
        \For{$j=1,2,\dots k$}
        \State $q_{j+1} = Aq_{j}$

        \For{ $i=1,2,\dots j$}
        \State $h_{ij}= q_{j+1}^{t}q_{i}$
        \State $q_{j+1} = q_{j+1} - h_{ij}q_{i}$
        \EndFor
        \State $h_{j+1,j}=\norm{q_{j+1}}$
        \State $q_{j+1} = \frac{q_{j+1}}{h_{j+1,j}}$
        \EndFor
        \State Find $y = \min_{y} \norm{\beta e_{1} - H_{m}y}$
        \State $x = Q_{k}y$
        \State \textbf{Stop} if the residual is smaller than the tolerance
        \EndFor
    \end{algorithmic}
\end{algorithm}

However, \ref{alg:gmres_init} doesn't present an efficient way of finding the residual in each iteration. To solve this problem and also to find a more efficient way of solving the least squares in \ref{eq:y_gmres}, we apply a transformation to $H_{k}$, turning it into a triangular matrix.

\section{Givens's Rotation}
%FIXME: maybe a little text about plane rotations and householder transformations

Givens's operator, $G(i,i+1)$, is a unitary matrix such that the column vector $a = Gb$ has the elements $a(i) = r \in \mathbb{R}$ and $a(i+1)=0$. It has a structure as in \ref{eq:givens}. The coefficients $c_{i},s_{i}$ only appear in the rows $i$ et $i+1$.

\begin{equation}\label{eq:givens}
    G(i,i+1)=
    \begin{bmatrix}
        1 &        &   &        &       &   &        &   \\
          & \ddots &   &        &       &   &        &   \\
          &        & 1 &        &       &   &        &   \\
          &        &   & c_{i}  & s_{i} &   &        &   \\
          &        &   & -s_{i} & c_{i} &   &        &   \\
          &        &   &        &       & 1 &        &   \\
          &        &   &        &       &   & \ddots &   \\
          &        &   &        &       &   &        & 1 \\
    \end{bmatrix}
\end{equation}
This operator offers a way to transform the columns in $H_{m}$, \textit{zeroing} the elements outside the main diagonal. Since a product of unitary operators is still unitary, \ref{eq:y_gmres} can be written as \ref{eq:after_givens}, where $R_{m}$ and $g_{m}$ are the results from the application of multiple Givens's operators to $H_{m}$ and $\beta e_{1}$.

\begin{equation}\label{eq:after_givens}
    y = \argmin_{y} \norm{\beta e_{1} - H_{m}y} = \argmin_{y} \norm{g_{m} - R_{m}y}
\end{equation}


Thus, the new problem \ref{eq:after_givens} can be solved with a simple backwards substitution. If $g_{m} = [\gamma_{1} \dots \gamma_{m+1}]^{t}$, an $m+1$ column vector, and $\{ R_{m} \}_{ij} = r_{ij}$ an $m+1$ by $m$ upper triangular matrix with $r_{ii} \neq 0$ and its last row filed with zeros, each element of $y_{m} = [y_{1} \dots y_{m}]$ is given by \ref{eq:triangular_system}.

\begin{align}\label{eq:triangular_system}
    \begin{split}
        \gamma_{k} &= \sum_{i=k}^{m} r_{ki} y_{i}\\
        y_{m} &= \frac{\gamma_{m}}{r_{mm}} \\
        y_{i} &= \frac{1}{r_{ii}} \left( \gamma_{i} - \sum_{j=i+1}^{m} r_{ij} y_{j}  \right)
    \end{split}
\end{align}

A simple algorithm to this end can be written as \ref{alg:triang_system}.

\begin{algorithm}
    \caption{Backwards substitution}\label{alg:triang_system}
    \begin{algorithmic}[1]
        \State $A \in \mathbb{K}^{n \times n}, \{ A \}_{ij} = a_{ij}$ and $b\in \mathbb{K}^{n}$
        \For{$k=n,n-1,\dots$}
        \State $y_{k} = b_{k}$
        \For{$j=n,n-1,\dots k+1$}
        \State $y_{k} = y_{k} - a_{kj}y_{j}$
        \EndFor
        \State $y_{k} = \frac{y_{k}}{a_{kk}}$
        \EndFor
    \end{algorithmic}
\end{algorithm}



It can be shown that $g_{m}$ also contains the residual of each iteration \cite{saad2003iterative}.

\begin{proof}

    Since it's an $m+1$ column vector, we have, with $\Xi_{m}$ being the necessary Givens's Rotations to make $H_{m}$ upper triangular \ref{eq:proof_residual}.


    \begin{equation}\label{eq:proof_residual}
        \norm{b - Ax_{m}} = \norm{Q_{m+1}^{t}(\beta e_{1} - H_{m}y_{m})} = \norm{\beta e_{1} - H_{m}y_{m}} = \norm{\Xi_{m}^{t} (g_{m} - R_{m}y_{m})}
    \end{equation}

    And since $\Xi_{m}^{t}$ is a rotation matrix, its unitary and $\norm{\Xi_{m}^{t} (g_{m} - R_{m}y_{m})}=\norm{g_{m} - R_{m}y_{m}}$.

    For any vector $y$, as the last line in \ref{eq:proof_residual} is field with zeros:

    \begin{equation}
        \norm{g_{m} - R_{m}y_{m}}_{2}^{2} = |\gamma_{m+1}|^{2} + \norm{(g_{m})_{1:m} - (R_{m})_{1:m,1:m} (y_{m})_{1:m}}_{2}^{2}
    \end{equation}

    Since in the minimization problem, the second term of the right side becomes zero(a triangular system), the residual has its value as $|\gamma_{m+1}|$, which gives a more efficient way to obtain the residuals during each iteration.
\end{proof}

\section{Inexact GMRES}



The most expensive part in the code is in the matrix-vector product \ref{alg:arnoldi}, line 4. Therefore, one approach to accelerate the iterations involves an approximation of $Aq $, instead of using the exact product, as shown in \ref{eq:aprox_Aq}.

\begin{equation}\label{eq:aprox_Aq}
    \mathcal{A}q = (A + E)q
\end{equation}

Where \textit{E} in \ref{eq:aprox_Aq} is a \textit{perturbation matrix} that changes with each iteration and will be written as $E_{k}$ for iteration k.

When we execute the inexact matrix-vector product, instead of the regular one, the left side of \ref{eq:init_arnoldi} must be changed by \ref{eq:new_projection}.


\begin{align} \label{eq:new_projection}
    \begin{split}
        [(A + E_{1})q_{1}, (A + E_{2})q_{2},\dots, (A + E_{k})q_{k}] &= Q_{k+1}H_{k}\\
        (A + \mathcal{E}_{k})Q_{k} &= Q_{k+1}H_{k}, \hspace{0.1in} \mathcal{E}_{k} = \sum_{i=1}^{k}E_{i}q_{i}q_{i}^{t}\\
        \mathcal{A}_{k}Q_{k} &= W_{k}
    \end{split}
\end{align}
Where $W_{m} = Q_{m+1}H_{m}$ from this point forward.

Now the subspace spawn by the vectors of $Q_{k}$ is not the Krylov's subspace $\mathcal{K}_{k}(A,b)$, but these are still orthonormal.  To see what kind of subspace our new $Q$ spans, \ref{eq:new_projection} is looked into in \ref{eq:q_basis}.

\begin{align}\label{eq:q_basis}
    \begin{split}
        (A + E_{k})q_{k}=\mathcal{A}_{k} q_{k} = h_{1,k}q_{1} + h_{2,k}q_{2} + \dots + h_{k+1,k}q_{k+1}
    \end{split}
\end{align}

For $k=1$, we have that $q_{2}$ is a combination of the vectors $\mathcal{A}_{1}b$ and $b$ (since $q_{1}$ = b). For $k=2$ we see that $q_{3}$ is a combination that involves $\mathcal{A}_{2} \mathcal{A}_{1}b$ and so forth.

Expression \ref{eq:new_projection} then shows that $Q_{k}$ becomes a basis for a new Krylov's subspace:

\begin{equation}
    \mathcal{K}_{k}(A+\mathcal{E}_{k},b)= \spn \{b,\mathcal{A}_{1}b,\dots, \mathcal{A}_{k}\dots\mathcal{A}_{1}b \}    
\end{equation}
 

Made by a large perturbation in $A$, that gets updated in each iteration.

%FIXME: I think it's more clear, but we'll read it later for confirmation
A new distinction should also be made between the two types of residuals appearing in the process. We denote by $r_{k}$ the exact residual of an iteration, defined as $r_{k} = \norm{b - Ax_{k}}$, but too expensive to calculate every time. And $\tilde{r}_{k}=\norm{b-AQ_{k}y_{k}}$, the one that will really be calculated throughout the algorithm. This is analogous to what was done in the conventional GMRES, where the residual being computed each time was $\norm{b-AQ_{k}y_{k}} = \norm{\beta e_{1} - H_{k}y_{k}}$, the difference being that now these two different residuals are \textbf{not} the same, as we will see below in \ref{eq:res_relation}.

% A definition for both and a measure of how distant they are is in \ref{eq:res_relation}.

\begin{align}\label{eq:res_relation}
    \begin{split}
        r_{k} &= r_{0} - AQ_{k}y_{k}\\
        &= r_{0} - (Q_{k+1}H_{k} - [E_{1}q_{1},\dots , E_{k}q_{k}])y_{k}\\
        &= \tilde{r}_{k} +[E_{1}q_{1},\dots , E_{k}q_{k}]y_{k}\\
        \rightarrow \delta_{k} &= \norm{r_{k} - \tilde{r}_{k}}  = \norm{[E_{1}q_{1},\dots , E_{k}q_{k}]y_{k}}
    \end{split}
\end{align}

Considering $y_{k} = [\eta_{1}^{(k)} \dots \eta_{k}^{(k)} ] $, upper index to clarify the iteration, an upper bound for $\delta_{k}$ can be found, but before we go through \ref{eq:res_rightside}.

\begin{align}\label{eq:res_rightside}
    \begin{split}
        \norm{[E_{1}q_{1},\dots , E_{k}q_{k}]y_{k}} & = \norm{\sum_{i=1}^{k}E_{i}q_{i}\eta^{(k)}_{i}}\\
        \norm{\sum_{i=1}^{k}E_{i}q_{i}\eta^{(k)}_{i}} & \leq \sum_{i=1}^{k}\norm{E_{i}}\norm{q_{i}\eta^{(k)}_{i}}\\
        \norm{\sum_{i=1}^{k}E_{i}q_{i}\eta^{(k)}_{i}} & \leq \sum_{i=1}^{k}\norm{E_{i}}|\eta^{(k)}_{i}|
    \end{split}
\end{align}

We use the fact that $q_{i}$ are orthonormal between the last two lines. The bound on $\delta_{k}$ is then found in \ref{eq:borne_delta}.


\begin{equation}\label{eq:borne_delta}
    \delta_{k} = \norm{r_{k} - \tilde{r}_{k}} \leq \sum_{i=1}^{k} \norm{E_{i}} \norm{\eta_{i}^{(k)}}
\end{equation}

\ref{eq:borne_delta} tells us that in order to keep both residuals close, either the perturbation of $A$, somewhat measured by $\norm{E_{i}}$, or the elements of $y_{i}$ should be kept small. Since we expect to use more \textit{relaxed} approximations of $A$ as the iterations go on, a greater tolerance in $E_{k}$ could be compensated with a sufficiently small $y_{k}$.


The problem is $y_{k}$ is only found after the construction of $E_{k}$, so an upper bound must be also found for its value.

We consider now $\Xi_{k} =G(k,k+1) G(k-1,k) \dots G(1,2)$, where each $G$ represents a Givens rotation as shown in \ref{eq:givens}. So $\Xi_{k}$ is the unitary matrix(since all Givens rotations are themselves unitary) that transforms $H_{k}$ into an upper triangular matrix.

Since $\norm{H_{k}y - \beta e_{1}}=\norm{\Xi_{k}(H_{k}y - \beta e_{1})} $, the application of $\Xi_{k}$ in the norm $ \norm{H_{k}y - e_{1}\beta} $ transforms the problem into a minimization of a triangular linear system with non-zero diagonal elements in its coefficient matrix:
\begin{equation}
    y_{k} =\argmin_{y \in \mathbb{C}^{k}} \norm{\Xi_{k}H_{k}y_{k} - \Xi_{k}e_{1}\beta}= \argmin_{y \in \mathbb{C}^{k}} \norm{R_{k}y_{k} - g_{k}}
\end{equation}



Knowing $y_{k}$ is the solution of this triangular system, that can be solved by backwards substitution, we have \ref{eq:y_limit_start}:


\begin{align}\label{eq:y_limit_start}
    \begin{split}
        R_{k}y_{k}&=g_{k}
    \end{split}
\end{align}

Both sides of the equation above are column vectors of $k+1$ elements, since we are still considering $R_{k}$ with its last line filled with zeros. If we denote $\tilde{R_{k}} = (R_{k})_{1:k,1:k}$ and $\tilde{g_{k}}=(g_{k})_{1:k}$, \ref{eq:y_limit_start} can also be written as:

\begin{equation}\label{eq:y_limit_startrest}
    y_{k} = \tilde{R_{k}}^{-1}\tilde{g_{k}}
\end{equation}

Since $\tilde{R_{k}}$, the transformation of a Hessenberg matrix by a series of Givens rotations, is upper triangular and has no line full of zeros, then its inverse also is upper triangular.
Being an upper triangular matrix, the first $i-1$ elements of its i-th line are zeros, so using Matlab index notation in \ref{eq:R_elements}.

\begin{equation}\label{eq:R_elements}
    (\tilde{R_{k}}^{-1})_{i,1:k}\tilde{g_{k}}_ = (\tilde{R_{k}}^{-1})_{i,i:k}\tilde{g_{k}}
\end{equation}

Using this last result in \ref{eq:y_limit_startrest} gives \ref{eq:y_limit}.

\begin{align}\label{eq:y_limit}
    \begin{split}
        |\eta^{(k)}_{i}| &= \norm{(\tilde{R_{k}}^{-1})_{i,i:k}(g_{k})_{i:k}}\\
        |\eta^{(k)}_{i}| &\leq \norm{e_{k}\tilde{R_{k}}^{-1}} \norm{(g_{k})_{i:k}}\\
        |\eta^{(k)}_{i}| &\leq \norm{e_{k}\tilde{R_{k}}^{-1}} \norm{(g_{k})_{i:k}}\\
    \end{split}
\end{align}

Since $\norm{e_{k}\tilde{R_{k}}^{-1}} \leq \norm{\tilde{R_{k}}^{-1}} = \sigma_{k}(H_{k})^{-1}$ and $\norm{(g_{k})_{i:k}} \leq \norm{\tilde{r}_{i-1}}$ \cite{simoncini2003theory}, the bound is given by \ref{eq:bound_yinexact}.

\begin{equation}\label{eq:bound_yinexact}
    \norm{\eta_{i}^{(k)}} \leq \frac{1}{\sigma_{k}(H_{k})} \norm{\tilde{r}_{i-1}}
\end{equation}

Using both results of \ref{eq:bound_yinexact} and \ref{eq:borne_delta} gives the results in \ref{eq:boundE_intermediate}. Thus, a sufficient condition for $\delta_{k} \leq \epsilon$ is given by \ref{eq:bound_E}.


\begin{equation}\label{eq:boundE_intermediate}
    \delta_{k} \leq \sum_{i=1}^{k} \frac{\norm{E_{i}}}{\sigma_{k}(H_{k})}\norm{\tilde{r}_{i-1}}
\end{equation}

\begin{equation}\label{eq:bound_E}
    \norm{E_{i}} \leq \frac{\sigma_{k}(H_{k})\epsilon}{k\norm{\tilde{r}_{i-1}}}
\end{equation}

Since $H_{k}$ is also one of the matrices constructed throughout the method, a workaround is necessary to apply these bounds in a practical situation. Either using an estimation of $\sigma_{k}(H_{k})$ with the singular values of $A$ or grouping all uncalculated terms in an $\ell_{k}$ that will be estimated empirically \cite{simoncini2003theory}, obtaining \ref{eq:boundE_final}.

\begin{equation}\label{eq:boundE_final}
    \norm{E_{i}} \leq \ell_{k} \frac{1}{\norm{\tilde{r}_{i-1}}} \epsilon
\end{equation}

It has been noted in \cite{simoncini2003theory} that among the initial bounds, some aren't really sharp, mainly \ref{eq:res_rightside} and \ref{eq:bound_yinexact}, and further empirical analysis of these bounds could show a better theoretical bound can be found for both.

In \cite{simoncini2003theory}(section 5), it also has been shown that $\sigma_{k}(H_{k})$ in \autoref{eq:bound_E} can be approximated by $\sigma_{n}(A)$, the smallest singular value of $A$, through the bound:

\begin{equation}
    \sigma_{k}(H_{k}) \leq \sigma_{n}(A) - \norm{E_{1}q_{1}, E_{2}q_{2}, \dots, E_{k}q_{k}}
\end{equation}

Giving another way to evalue $\ell_{k}$ without the need of calculating $\sigma_{k}(H_{k})$ at each iteration. A plot of these bounds for the cavity problem will be studied later.

It should also be noted that $\tilde{r}_{k}$, after the transformation of $H_{k}$ into an upper triangular matrix, is also found in the $i+1$-th element of $g_{m}$ in \ref{eq:y_limit_start}. The demonstration follow the same proof as for $g_{m}$ in \ref{eq:after_givens}, given that $y_{m}$ is a solution to a linear system that involves an upper triangular matrix.

The remaining theory in this report also explains the basics of Hierarchical Matrices, the structure that will be used to compress the matrices used in the algorithm, since $A$ appears in the discretization of integral operators and uses large dimensions.
It's also though these structures each $\mathcal{A}_{k}$ will be made. As it will be explained later, at each iteration an $E_{k}$ will be indirectly constructed during the inexact product $\mathcal{A}_{k}q$, mainly using the residues that appear during this structure's construction, in the iterations of the \textit{ACA Method}.